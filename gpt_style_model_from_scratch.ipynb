{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# A google colab with my own implementation of a decoder only model\n",
        "\n"
      ],
      "metadata": {
        "id": "49BW5Cmyfb6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's use tokenizers only to create a vocabulary with a tinyshakespeare dataset\n"
      ],
      "metadata": {
        "id": "FlWw7hkGfqIu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlGYhm93K9CN"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's import the libraries needed\n",
        "\n",
        "*   For this script we will only use torch, tokenizer from huggingface, and load_dataset to manipulate our data easier\n",
        "\n",
        "We will reproduce the only decoder part of the original paper\n",
        "\"Attention is all you need\"\n"
      ],
      "metadata": {
        "id": "qscPzWMyf_zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "50y7YCCv2EGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's start by creating a vocabulary with tinyshakespeare and load the data using load_dataset"
      ],
      "metadata": {
        "id": "NLq-djQibnZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's create a custom Tokenizer to train a tokenizer from scratch with the tinyshakespeare\n",
        "\n",
        "I created a custom tokenizer following these criterias:\n",
        "\n",
        "1. The Normalizer\n",
        "Before we analyze the text, we must clean it. If we have the word \"Apple\" and \"apple\", the computer thinks they are different things.\n",
        "\n",
        "\n",
        "*   Lowercase: Converts everything to lowercase so the model doesn't have to learn \"The\" and \"the\" separately.\n",
        "* NFD Unicode & Accents: It breaks characters into parts (e.g., é becomes e + ´).\n",
        "* We then strip the accent, leaving just the base e. This makes the tokenizer much more robust across different languages.\n",
        "\n",
        "2. The Pre-Tokenizer\n",
        "We can't let the WordPiece model just start merging letters randomly. If we didn't have this step, the model might try to merge the end of one word with the start of the next (e.g., \"blue\" and \"apple\" becoming \"ueap\"). The Pre-Tokenizer acts as a boundary setter. It usually splits by whitespace and punctuation, creating a list of \"candidate words\" that the WordPiece algorithm is allowed to work on.\n",
        "\n",
        "3. The WordPiece Model\n",
        "This is where the magic happens. WordPiece is a subword tokenization algorithm. Its goal is to find a balance between character-level and word-level tokenization. If we use whole words, our vocabulary becomes millions of words long (very expensive). If we use just characters, the sequences become too long and lose meaning. The WordPiece Solution: It keeps common words whole (like the) but breaks rare words into meaningful chunks. It uses a likelihood-based approach: it merges characters into a subword only if that merge makes the training data more predictable. The ## Prefix: This is a flag. It tells the model: \"This piece (like ##ly) is not a new word; it's a continuation of the previous one.\"\n",
        "\n",
        "4. The Post-Processor\n",
        "Once the text is broken into subwords, we need to format it for the specific AI architecture (like BERT). [CLS] (Classification): Added to the very beginning. The model uses the \"thought\" at this position to understand the meaning of the whole sentence. [SEP] (Separator): Added to the end of sentences so the model knows where one idea ends and another begins."
      ],
      "metadata": {
        "id": "pMJyzp1zTqag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer:\n",
        "    \"\"\"\n",
        "    A configurable tokenizer pipeline using the WordPiece algorithm.\n",
        "\n",
        "    This class encapsulates the full lifecycle of a tokenizer, from\n",
        "    normalization and pre-tokenization to training and persistence.\n",
        "\n",
        "    Attributes:\n",
        "        model (tokenizers.models.Model): The underlying tokenization model.\n",
        "        tokenizer (tokenizers.Tokenizer): The high-level tokenizer instance.\n",
        "        trainer (tokenizers.trainers.Trainer): The trainer object containing\n",
        "            hyperparameters like vocab size and special tokens.\n",
        "        special_tokens (list): A list of reserved tokens (e.g., [PAD], [CLS]).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_vocab: int = 30522,\n",
        "        model=None,\n",
        "        normalizer=None,\n",
        "        pre_tokenizer=None,\n",
        "        trainer=None,\n",
        "        special_tokens=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the pipeline with standard defaults for WordPiece (BERT-style).\n",
        "\n",
        "        Args:\n",
        "            d_vocab (int): The target vocabulary size for the trained model.\n",
        "            model: Optional model instance. Defaults to WordPiece.\n",
        "            normalizer: Optional normalization sequence. Defaults to NFD,\n",
        "                Lowercase, and StripAccents for general text.\n",
        "            pre_tokenizer: Optional pre-tokenizer. Defaults to Whitespace.\n",
        "            trainer: Optional trainer instance.\n",
        "            special_tokens: List of strings to be reserved in the vocabulary.\n",
        "        \"\"\"\n",
        "        self.special_tokens = special_tokens or \\\n",
        "            [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "\n",
        "        self.unk_token = self.special_tokens[0] # Assumes UNK is the first element\n",
        "\n",
        "        self.model = model or models.WordPiece(unk_token=self.unk_token)\n",
        "        self.tokenizer = Tokenizer(self.model)\n",
        "\n",
        "        self.tokenizer.normalizer = normalizer or normalizers.Sequence([\n",
        "            normalizers.NFD(),\n",
        "            normalizers.Lowercase(),\n",
        "            normalizers.StripAccents()\n",
        "        ])\n",
        "\n",
        "        self.tokenizer.pre_tokenizer = pre_tokenizer or pre_tokenizers.Whitespace()\n",
        "\n",
        "        self.trainer = trainer or trainers.WordPieceTrainer(\n",
        "            vocab_size=d_vocab,\n",
        "            special_tokens=self.special_tokens,\n",
        "            limit_alphabet=1000,\n",
        "            initial_alphabet=list(string.ascii_letters + string.digits),\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "    def train(self, dataset, batch_size=1000):\n",
        "        \"\"\"\n",
        "        Trains the tokenizer model on a provided dataset using a generator.\n",
        "\n",
        "        This method uses a lazy-loading approach to handle datasets that\n",
        "        exceed available RAM.\n",
        "\n",
        "        Args:\n",
        "            dataset: A dataset object indexable via dataset[i:j].\n",
        "            batch_size (int): Number of text samples per iteration.\n",
        "        \"\"\"\n",
        "        def get_training_corpus():\n",
        "            for i in range(0, len(dataset), batch_size):\n",
        "                yield dataset[i : i + batch_size][\"text\"]\n",
        "\n",
        "        self.tokenizer.train_from_iterator(\n",
        "            get_training_corpus(),\n",
        "            trainer=self.trainer\n",
        "        )\n",
        "\n",
        "        cls_id = self.tokenizer.token_to_id(\"[CLS]\")\n",
        "        sep_id = self.tokenizer.token_to_id(\"[SEP]\")\n",
        "\n",
        "        self.tokenizer.post_processor = processors.TemplateProcessing(\n",
        "            single=\"[CLS] $A [SEP]\",\n",
        "            pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "            special_tokens=[(\"[CLS]\", cls_id), (\"[SEP]\", sep_id)]\n",
        "        )\n",
        "\n",
        "    def encode(self, text: str, add_special_tokens: bool = True):\n",
        "        \"\"\"\n",
        "        Encodes a single string of text into token IDs.\n",
        "\n",
        "        Returns:\n",
        "            tokenizers.Encoding: An object containing token IDs, offsets, etc.\n",
        "        \"\"\"\n",
        "        return self.tokenizer.encode(text, add_special_tokens=add_special_tokens)\n",
        "\n",
        "    def decode(self, ids: list):\n",
        "        \"\"\"\n",
        "        Decodes a list of token IDs back into a string.\n",
        "        \"\"\"\n",
        "        return self.tokenizer.decode(ids)\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"\n",
        "        Saves the trained tokenizer to a JSON file for deployment.\n",
        "\n",
        "        Args:\n",
        "            path (str): File system path to save the configuration.\n",
        "        \"\"\"\n",
        "        self.tokenizer.save(path)\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Get the vocabulary used for training.\n",
        "        \"\"\"\n",
        "        return self.tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "RStNY8_wbyvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's train the model to learn the vocabulary we have in our dataset"
      ],
      "metadata": {
        "id": "zCqEKCu1WTve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customTokenizer = CustomTokenizer()\n",
        "customTokenizer.train(dataset)\n",
        "customTokenizer.encode('Looks like this is currently working').tokens # a simple test!"
      ],
      "metadata": {
        "id": "mI-WewTwWS35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customTokenizer.encode('Looks like this is currently working').ids # Let's check the ids in the new vocab!"
      ],
      "metadata": {
        "id": "by2GcFJ8mIXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customTokenizer.get_vocabulary() # Let's check the new vocabulary"
      ],
      "metadata": {
        "id": "22-Snq7LpWj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's define the parameters for our model\n",
        "\n",
        "- d_model (Model Dimensionality): $64$ This is the size of the internal feature vectors used throughout the model. Every token is represented by a 64-dimensional vector, and this size dictates the width of the main information pathway in the transformer layers.\n",
        "- seq_len (Sequence Length / Context Window): $32$ The maximum number of tokens the model can process simultaneously. This is the length of the input sequence the model's positional encodings and attention mechanism are designed to handle.\n",
        "- batch_size (Batch Size): $16$ The number of independent training sequences processed in parallel before the model's weights are updated.\n",
        "- num_heads (Number of Attention Heads): $4$ The number of independent parallel sub-spaces the attention mechanism uses. Each head focuses on different parts of the context, allowing the model to capture varied relationships within the sequence.\n",
        "- num_layers (Number of Decoder Blocks): $4$ The depth of the model, specifying how many times the core attention-FFNN structure is repeated.\n",
        "- d_ff (Feed-Forward Inner Dimension): $4 \\times d\\_model$ ($256$) The size of the hidden layer in the position-wise Feed-Forward Network (FFNN). This intermediate size is typically set to $4 \\times d\\_model$.\n",
        "- learning_rate ($\\eta$): $1e-3$ The main step-size hyperparameter for the Adam optimizer, controlling how aggressively the model adjusts its weights during training.\n",
        "- max_iters (Maximum Training Iterations): $100000$ The total number of optimization steps (batches) the training loop will execute.\n",
        "- eval_interval (Evaluation Interval): $500$ The number of training steps after which the model will pause training to evaluate its current performance on the validation set.\n",
        "\n",
        "You can adjust the parameters that make sense to you!"
      ],
      "metadata": {
        "id": "9WjEH1WNjoNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 64\n",
        "seq_len = 32\n",
        "batch_size = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "d_ff = 4 * d_model # 4 * d_model is a common rule of thumb\n",
        "learning_rate = 1e-3\n",
        "max_iters = 100000\n",
        "eval_interval = 500\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "JwXzXYfwSq-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's create the classes for our decoder only model\n",
        "\n",
        "This is the most critical section of this notebook. Based on the seminal paper \"Attention Is All You Need,\" I have isolated and reproduced the Decoder-only architecture (the foundation for modern LLMs like GPT).\n",
        "\n",
        "To make this complex system accessible, I have heavily documented the classes with \"step-by-step\" commentary. In this session, we will perform a \"surgical\" breakdown of every component, mapping the raw code directly back to the architectural diagrams from the original paper.\n"
      ],
      "metadata": {
        "id": "UTdORFr8Zpal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddAndNorm(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, input, output):\n",
        "        return self.norm(input + output)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=4):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        positions = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = 10000**(torch.arange(0,d_model, 2)/d_model)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(positions/div_term)\n",
        "        pe[:, 1::2] = torch.cos(positions/div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        return embeddings + self.pe[:embeddings.shape[1], :]\n",
        "\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, in_dim, inner_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.ffnn = nn.Sequential(\n",
        "            nn.Linear(in_dim, inner_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(inner_dim, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.ffnn(input)\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.linear(input)\n",
        "\n",
        "class Softmax(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.softmax(input)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, bias=False):\n",
        "        super().__init__()\n",
        "        # Ensure d_model is divisible by num_heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Store model dimensions\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Linear layers for Q, K, V, and output\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=bias)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=bias)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=bias)\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias=bias)\n",
        "\n",
        "        # Softmax for attention scores\n",
        "        self.P = nn.Softmax(dim=-1)\n",
        "\n",
        "        # Placeholders for intermediate values (great for debugging)\n",
        "        self.Q = None\n",
        "        self.K = None\n",
        "        self.V = None\n",
        "        self.scores = None\n",
        "        self.attention = None\n",
        "        self.output = None\n",
        "\n",
        "    def get_linear_projections(self, input):\n",
        "        \"\"\"Projects input into Q, K, V.\"\"\"\n",
        "        self.Q = self.W_q(input)\n",
        "        self.K = self.W_k(input)\n",
        "        self.V = self.W_v(input)\n",
        "\n",
        "    def get_heads(self, batch_size, seq_len):\n",
        "        \"\"\"Reshapes Q, K, V to have multiple heads.\"\"\"\n",
        "        self.Q = self.Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        self.K = self.K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        self.V = self.V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def get_scores(self):\n",
        "        \"\"\"Calculates scaled dot-product attention scores.\"\"\"\n",
        "        self.scores = torch.matmul(self.Q, self.K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    def apply_masking(self, seq_len):\n",
        "        \"\"\"Applies a causal mask to the scores.\"\"\"\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(self.scores.device)\n",
        "        self.scores = self.scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "    def apply_softmax(self):\n",
        "        \"\"\"Applies softmax to the scores to get attention weights.\"\"\"\n",
        "        self.attention_weights = self.P(self.scores)\n",
        "\n",
        "    def get_attention(self):\n",
        "         \"\"\"Computes the attention output by multiplying weights with V.\"\"\"\n",
        "         self.attention = torch.matmul(self.attention_weights, self.V)\n",
        "\n",
        "    def get_output(self, batch_size, seq_len):\n",
        "        \"\"\"Concatenates heads and applies the final linear layer.\"\"\"\n",
        "        reshaped = self.attention.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        self.output = self.W_o(reshaped)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Executes the full multi-head attention mechanism.\n",
        "        This single method calls all the helper methods in the correct order.\n",
        "        \"\"\"\n",
        "        # Get dimensions from the input tensor\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # 1. Get Q, K, V projections\n",
        "        self.get_linear_projections(x)\n",
        "\n",
        "        # 2. Split into multiple heads\n",
        "        self.get_heads(batch_size, seq_len)\n",
        "\n",
        "        # 3. Calculate attention scores\n",
        "        self.get_scores()\n",
        "\n",
        "        # 4. Apply causal mask (for decoder self-attention)\n",
        "        self.apply_masking(seq_len)\n",
        "\n",
        "        # 5. Apply softmax to get weights\n",
        "        self.apply_softmax()\n",
        "\n",
        "        # 6. Get weighted sum of Values\n",
        "        self.get_attention()\n",
        "\n",
        "        # 7. Concatenate heads and get final output\n",
        "        self.get_output(batch_size, seq_len)\n",
        "\n",
        "        # Return the final output tensor\n",
        "        return self.output\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff=32, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initializes a single Decoder Block.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the model (embedding size).\n",
        "            num_heads (int): The number of attention heads.\n",
        "            d_ff (int): The inner dimension of the feed-forward network.\n",
        "                        Typically 4 * d_model.\n",
        "            dropout (float): The dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # First sub-layer: Masked Multi-Head Attention\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.add_norm1 = AddAndNorm(d_model)\n",
        "\n",
        "        # Second sub-layer: Feed-Forward Network\n",
        "        self.ffnn = FFNN(d_model, d_ff, d_model)\n",
        "        self.add_norm2 = AddAndNorm(d_model)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Compute attention\n",
        "        attn_output = self.attention(x)\n",
        "\n",
        "        # 2. Apply dropout and the first residual connection + normalization\n",
        "        x = self.add_norm1(x, self.dropout(attn_output))\n",
        "\n",
        "        # --- Feed-Forward Sub-layer ---\n",
        "        # 1. Compute feed-forward output\n",
        "        ff_output = self.ffnn(x)\n",
        "        # 2. Apply dropout and the second residual connection + normalization\n",
        "        x = self.add_norm2(x, self.dropout(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "class MyGPT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        seq_len,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        d_ff,\n",
        "        vocab_size,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, seq_len)\n",
        "\n",
        "        self.decoder_blocks = nn.Sequential(\n",
        "           *[DecoderBlock(d_model, num_heads, d_ff) for layer in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.lm_head = Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tokens, targets=None):\n",
        "        embeddings = self.token_embedding_table(tokens)\n",
        "        pos_embed = self.positional_encoding(embeddings)\n",
        "        x = self.decoder_blocks(pos_embed)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets != None:\n",
        "            B, T, C = logits.shape\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Generates new tokens by iteratively predicting the next token.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): A tensor of shape (B, T) containing the\n",
        "                                starting token indices (the prompt).\n",
        "            max_new_tokens (int): The maximum number of tokens to generate.\n",
        "            temperature (float): Softmax temperature for controlling randomness.\n",
        "            top_k (int, optional): Sample from the top K most likely tokens.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The input tensor with the newly generated tokens\n",
        "                          appended, shape (B, T + max_new_tokens).\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # 1. Crop the context if it's longer than seq_len\n",
        "            # This ensures the input to the model doesn't exceed the max length\n",
        "            # the model was trained with (self.positional_encoding's max_len).\n",
        "            idx_cond = idx[:, -self.positional_encoding.pe.shape[0]:]\n",
        "\n",
        "            # 2. Get predictions (logits)\n",
        "            # The model's forward method gives logits for all tokens in the sequence.\n",
        "            logits, _ = self(idx_cond) # Logits shape: (B, T_cond, C)\n",
        "\n",
        "            # 3. Focus only on the *last* token's prediction (the next one)\n",
        "            # The logits for the last token are what we use to sample the *next* token.\n",
        "            logits = logits[:, -1, :] # Logits shape: (B, C)\n",
        "\n",
        "            # 4. Apply temperature to logits\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # 5. Optional: Apply Top-K sampling\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "            # 6. Apply softmax to convert logits to probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # 7. Sample the next token from the probability distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # Shape (B, 1)\n",
        "\n",
        "            # 8. Append the sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "BPiUztriZl3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's encode all the data into tokens\n",
        "\n",
        "In this part we will take all the data to train our model. Before passing the data to our decoder model we have to covert words into tokens(ids).\n"
      ],
      "metadata": {
        "id": "gYmgevA4qe_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(example):\n",
        "    encoding = customTokenizer.encode(example[\"text\"])\n",
        "    return {\"ids\": encoding.ids}\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_fn,\n",
        "    remove_columns=[\"text\"],\n",
        "    desc=\"Tokenizing dataset\"\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GXgpJOWNowdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's train our decoder only model\n",
        "\n",
        "Now based on the parameters previously defined, let's proceed to train our decoder only model."
      ],
      "metadata": {
        "id": "1ajwNDmJrdL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = []\n",
        "\n",
        "for item in tokenized_dataset:\n",
        "    all_ids.extend(item[\"ids\"])\n",
        "all_ids_tensor = torch.tensor(all_ids, dtype=torch.long)\n",
        "\n",
        "def get_batch(batch_size, seq_len):\n",
        "    # Randomly pick starting indices\n",
        "    ix = torch.randint(0, len(all_ids_tensor) - seq_len - 1, (batch_size,))\n",
        "\n",
        "    x = torch.stack([all_ids_tensor[i : i + seq_len] for i in ix])\n",
        "    y = torch.stack([all_ids_tensor[i + 1 : i + seq_len + 1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "model = MyGPT(\n",
        "    d_model = d_model,\n",
        "    seq_len = seq_len,\n",
        "    num_heads = num_heads,\n",
        "    num_layers = num_layers,\n",
        "    d_ff = d_ff,\n",
        "    vocab_size = customTokenizer.get_vocabulary(),\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "for iter in range(max_iters):\n",
        "    x, y = get_batch(batch_size, seq_len)\n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    logits, loss = model(x, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Let's print the loss every 100 iterations to see how our model training\n",
        "    # is going.\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Step {iter}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "YB6_eWkAqVzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's create text from a prompt\n",
        "Let's make sure the model is in testing model by calling the method .eval()\n"
      ],
      "metadata": {
        "id": "Xbzlc07AuWER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "prompt_text = \"the king said\"\n",
        "start_ids = customTokenizer.encode(prompt_text).ids\n",
        "context = torch.tensor([start_ids], dtype=torch.long, device=device)\n",
        "max_new_tokens = 200 # Let's say we want to generate 100 more tokens\n",
        "\n",
        "generated_tokens = model.generate(\n",
        "    idx=context,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    temperature=0.8,  # A temperature slightly less than 1.0 (e.g., 0.8)\n",
        "                      # often gives more coherent results\n",
        "    top_k=50          # Restrict sampling to the top 50 most likely tokens\n",
        ")\n",
        "\n",
        "# Decode the generated tokens\n",
        "output_tokens = generated_tokens[0].tolist()\n",
        "generated_text = customTokenizer.decode(output_tokens)\n",
        "\n",
        "# Print the final result\n",
        "print(\"--- Generated Text ---\")\n",
        "print(generated_text)\n",
        "print(\"----------------------\")"
      ],
      "metadata": {
        "id": "3UrrCMsXqoiq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}